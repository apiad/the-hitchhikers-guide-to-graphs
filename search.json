[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "",
    "text": "Preface\nWhat do the World Wide Web, your brain, the corpus of scientific knowledge accumulated by all of humanity, the entire list of people you’ve ever met, and the city you live in have in common?\nThese are all very different types of things, from physical, to virtual, to social in nature, but they share a very important trait. They are all networks that establish relationships between some entities.\nThe World Wide Web is a network of interconnected computational resources, data, software, and hardware infrastructure. Your brain is a network of interconnected neurons. The accumulated human knowledge is also a network of interconnected ideas, as all discoveries depend upon prior knowledge, and unlock new discoveries. The city you live in is also an interconnected network of roads and buildings. And the people you know is also network, as many of them know each other, or know someone that knows someone you know.\nAs distinct as these things are, they all share common properties, by virtue of their networked nature. For example, you can think of how “close” two entities in this network are. The meaning of “distance” will be different if you’re considering physical networks (like roads) versus information networks (like published papers with citations to other papers) versus social networks (like your Facebook friends), but in all cases there is some sense in which some entities are “closer” together than others.\nWhat if we could study this abstract notion of networks, and understand their fundamental properties all at once?\nWelcome to graph theory.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#welcome-to-graph-theory",
    "href": "index.html#welcome-to-graph-theory",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "Welcome to Graph Theory!",
    "text": "Welcome to Graph Theory!\nTo study these seemingly disparate objects in the deepest possible way, we need an abstract concept that captures the fundamental notion of things connected together. In mathematics and computer science, they are called graphs. Graphs lie at the intersection of mathematics and computer science, and their study is one of the most fascinating branches of math, science, and human knowledge in general.\nThis book is a journey through the theory and applications of graphs, synthesizing the practical and the mathematical points of view. In each chapter, we will look at one specific problem and see how it can be translated into the language of graphs. Then we will design a computational strategy – an algorithm – to solve that problem, or at least to attempt an approximation – because some problems are too hard to solve them completely. In doing so, we will also look at the underlying theory that makes our solution work.\nIn the next few hundred pages, we will teach you almost everything we know about graphs, including their most intriguing mathematical properties, but also their most practical uses to solve real-world scientific, engineering, and social problems.\nThus, when you’ve finished reading this book, you will have a solid understanding and familiarity of graphs, similar to the level expected from a Computer Science graduate in most curricula out there. (And perhaps deeper in some niche topics that are usually not taught in a CS major.)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-this-book-is-and-what-is-not",
    "href": "index.html#what-this-book-is-and-what-is-not",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "What this book is (and what is not)",
    "text": "What this book is (and what is not)\nThis book is not a textbook for a computer science course. There are plenty of wonderful books you can use if you’re learning (or teaching) about graphs at college. The most essential of these books is Introduction to Algorithms1, which I strongly recommend you check out if you want a more traditional – and complete – introduction to, well, algorithms in general and graphs in particular. It is the best selling book in the history of computer science.\nThis book will cover both some of the simplest and some of the most advanced algorithms and concepts in graph theory, so there is no single one-semester course where all this content could be taught. Depending on your background and interests, this book will serve you in different ways.\nIf you are a Computer Science student, think of this book as a complement to all you will see in college. As you learn about graphs in different subjects, you can come to this book and find algorithms, theorems, and proofs suitable to the level of depth you’re exploring.\nIf you are a CS professor, this book can give you some cool ideas to introduce specific graph theorems and algorithms, along with clean and simple reference implementations. The explanations are also more intuitive than what you can find in more traditional textbooks. The book also features follow-up questions and problems that you can mix-and-match with your own to enrich your classes or exams.\nIf you are a software developer, you can use this book to complement your knowledge about graphs, or fill any gaps you may have from your background studies. You can also use it as a quick reference to many of the most used graph algorithms, and as an inspiration to introduce graph theory into your current projects, perhaps improving some of your work.\nIf you are a casual coder, this book will give you plenty of cool problems to play with and ideas to build some cool projects on your own. It may even give you the inspiration to go deeper into other computer science fields such as AI and optimization.\nAnd if you are none of the above, you can still enjoy this book. Some of the most beautiful ideas in math emerge naturally from graph theory. In any case, as I’ll try to convince you in this book, graphs are everywhere. Knowing more about them can only help you see the world around you with a new pair of glasses, and that’s always a good thing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis book assumes some basic knowledge of programming. If you have some experience with the Python programming language, you’ll find it easier to understand the code examples. However, even if you’ve never seen Python before, a minimal familiarity with any programming language will probably be more than enough, since Python is super intuitive – at least its small subset that we’ll use throughout this book. If you’re new to Python or need a refresher, check the Appendix B — A Primer on Python for a quick intro to the basic features of the language that we’ll need in this book.\nIf you have never, ever seen a programming language before, then the coding part of this book will be harder for you. However, you can still enjoy the intuitive and visual explanations and the mathematical proofs, and skip all the code. We will always present an informal explanation of any algorithms that we study. And if getting the most out of this book is the nudge you need to learn a little bit of coding, even better!\nRegarding math, you should be at least comfortable with high-school level, but no college-level math is necessary. Specially, no advanced calculus or algebra appears anywhere in this book. The majority of the proofs use only basic logic, and we’ll introduce any mathematical tool we need – such as strong induction or reductio ad absurdum – in the Appendix A — A Primer on Logic. That being said, some minimal previous exposure to discrete math won’t hurt you.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-read-this-book",
    "href": "index.html#how-to-read-this-book",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "How to read this book",
    "text": "How to read this book\nThis book is divided into several parts that are more or less independent. Except for 1  Introduction, all other chapters can be read roughly in any order, although the first few chapters introduce the most important algorithms. In some cases a later chapter may reference a previous result or algorithm, but in those cases you’ll find an explicit link to any required content.\nThus, unless you’ve already familiar with the basic notations and concepts in graph theory, I recommend you read the 1  Introduction first. Afterwards, you’re free to hop around and take any chapter that suits your interest.\n\n\n\n\n\n\nNote\n\n\n\nActually, we can model the problem of reading this book as a graph problem, where each chapter is a node, and the edges indicate dependencies. In this graph, you can quickly find what is the optimal way to read the book if you are only interested in some chapters, so that you are guarantee to cover all pre-requisites. We will solve this very problem in ?sec-toposort.\n\n\nEach part deals with solving problems of a similar nature, and is divided into chapters that grow in complexity. That means, say, the last chapter of Part 2 may be more advanced than the first chapter of Part 4.\nEach chapter is dedicated to solving one specific problem, introducing ideas and algorithms that emerge from our journey towards the solution. In each chapter, we will\n\nunderstand and mathematically formulate the problem,\nbuild intuitions to channel our ideas towards the solution,\nand discover the solution in the form of a theorem or an algorithm. If it’s an algorithm, we’ll build a reference implementation in Python as well.\n\nIntermixed between all of this, we’ll drop some theoretical sections that delve into the underlying math and provide some formal proofs. These sections will be clearly marked and are absolutely optional, so you are free to skip them, at least on a first read.\nAnd to keep things a bit on the fun side, instead of simply tackling dry, boring, abstract graph problems, we will frame our discussions in the context of the fictional city of “Graphtopia”, where all citizens are crazy about graphs and everything is done using graph theory.\nIn summary, you can read this book top-to-bottom, or you can skim and jump around as much as you want.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#complementary-source-code",
    "href": "index.html#complementary-source-code",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "Complementary source code",
    "text": "Complementary source code\nThis book comes with by a simple Python library called hitchhikers-graphs that contains all the code we’ll write and use throughout the book. You will find a short reference for this library in the Appendix C — The hitchhiker-graphs Python Package.\nThe library itself is open-source and can serve as a starting point for other projects, but it is not designed to be a production-ready solution for solving graph problems. It is merely an educational device, designed to help you truly understand graph theory. For this reason, all implementations are purposefully oversimplified and more care is given to clarity and simplicity than performance.\nThat being said, you’re free to download, modify, and use it as you see fit.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a-word-on-notation",
    "href": "index.html#a-word-on-notation",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "A word on notation",
    "text": "A word on notation\nThis book follows the standard mathematical notation for graphs that you can find anywhere, and we will learn that notation in due time, incorporating new concepts as we need them.\nMost of the math- and code-heavy parts are optional and clearly marked so you can decide whether to skip them or dive into them. This is how you identify them:\n\n\\(\\small(\\Sigma)\\) All sections or blocks marked with this symbol are math-heavy, and usually include proofs for theorems just discussed in the previous sections.\n\\(\\small[\\lambda]\\) All sections or block marked with this symbol are code-heavy, and most often include a Python implementation of an algorithm previously discussed.\n\nFor emphasis: although you can definitely skip these sections and still enjoy and get a lot out of this book, we strongly encourage you to, at least on a second read, give them a try. We’ve made our best to be as clear and intuitive as possible even in these more advanced sections.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "The Hitchhiker’s Guide to Graphs",
    "section": "",
    "text": "https://mitpress.mit.edu/9780262367509/introduction-to-algorithms/↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "What is a graph\nIntuitively, a graph is just a (finite) collection of elements called vertices (or sometimes nodes), connected via edges. Graph represents an abstract relation space, in which the edges define who’s related to whom, whatever the nature of that relation is.\nMathematically speaking, a graph is an object composed of two sets: one for the vertices, and another for the edges. In fact, edges themselves are sets of two vertices. There is nothing about an edge that matters beyond which are the two vertices it connects. (We will see this isn’t the case for some special classes of graphs, but we’ll only deal with them later.)\nGraphs are abstract objects, which we can visualize by drawing dots for vertices and lines for edges. An example of a simple graph is shown in Figure 1.1.\nFigure 1.1: An example of a graph, with 5 vertices and 7 edges.\nThis graph is composed of 5 vertices (labeled as \\(a, b, c, d, e\\)) and 7 edges. Of course, there is nothing intrinsic to names or the exact location of the vertices in the drawing. Except in very concrete cases – such as when a graph represents a geometrical or geographical object – the layout of a graph is arbitrary, and the same graph can be represented in an infinite number of ways.\nFor instance, Figure 1.2 are two different representations of the very same graph.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-a-graph",
    "href": "intro.html#what-is-a-graph",
    "title": "1  Introduction",
    "section": "",
    "text": "(a) A flat layout\n\n\n\n\n\n\n\n\n\n\n\n(b) Simulating a “3d” layout\n\n\n\n\n\n\n\nFigure 1.2: Two different representations of the same graph.\n\n\n\n\n\n\n\n\n\n\nIntersecting edges\n\n\n\nThe first representation on Figure 1.2 (a) is drawn without no edges intersecting each other, while Figure 1.2 (b) is not. Can we draw any graph without intersecting edges? This simple question leads us to the beautiful domain of planar graphs. We’ll deal this with much later. Let’s see first what graphs are!\n\n\n\nFormalizing graphs \\(\\small(\\Sigma)\\)\n\n\n\n\n\n\nOptional math sections\n\n\n\nSections with a \\(\\small(\\Sigma)\\) symbol are mathematical definitions or proofs that you can safely ignore if you don’t care about the formalization.\n\n\nWe are ready to use the tools of logic and set theory to formally define the concept of a graph in the language of mathematics.\n\n\nDefinition 1.1 (Graph) A graph \\(G = (V, E)\\) is a collection of two sets, a set of vertices \\(V\\), and a set of edges \\(E \\subseteq V \\times V\\).\n\n\n(We’ll often use the notation \\(V(G)\\) and \\(E(G)\\) for clarity to indicate the source graph \\(G\\). This is useful when juggling with multiple graphs and subgraphs.)\nLet’s unpack this definition. Take a look at Figure 1.1 one more time.\nIn the language of mathematics, this graph is defined by \\(G = (V, E)\\), where the vertices are defined by: \\[V = \\{ a, b, c, d, e \\}\\] and the edges are defined by: \\[E = \\{ (a, e), (a, b), (b, e), (b, c), (b, d), (c, d), (c, e), (d, e) \\}\\]\nTo prevent the unnecessary proliferation of commas and parentheses, we’ll often denote the edge \\((u, v)\\) with \\(uv\\). You can thank us later.\nThe most important characteristic of a vertex is its degree, which basically measures the number of edges between this vertex and any other in the graph. The degree of a vertex \\(v\\) is denoted by \\(d(v)\\). Thus, in our example, \\(d(a) = 2\\), because only the edges \\(ab\\) and \\(ae\\) connect \\(a\\) to another vertex. In contrast, the remaining vertices have degree \\(3\\).\nWe call the set of vertices adjacent to an arbitrary vertex \\(v\\) its neighborhood, denoted by \\(N(v)\\). Thus, the neighborhood of vertex \\(c\\) is the set of vertices \\(\\{ b, d, e \\}\\).\nBy definition, the degree of a vertex is the size of its neighborhood; that is, \\(d(v) = |N(v)|\\).\n\n\nProgramming with graphs \\(\\small[\\lambda]\\)\n\n\n\n\n\n\nOptional coding sections\n\n\n\nSections with a \\(\\small[\\lambda]\\) symbol are related to coding, and you can safely ignore them if you don’t care about the programming part.\n\n\nComputationally speaking, you can think of a graph as an abstract class (or an interface in languages that support that notion) that provides two key operations: listing all nodes, and determining if two nodes are connected.\n\n\n\n\n\n\n\ngraphs/core.py  See on Github\n\n\n\nclass Graph:\n    def nodes(self):\n        raise NotImplemented\n\n    def adjacent(self, x, y) -&gt; bool:\n        raise NotImplemented\n\n    # ... rest of class Graph\n\n\n\nYou may be wondering, what if we want to modify the graph? While that makes total sense in some applications, since we want to use this graph abstraction as flexibly as possible. (E.g., as a read only interface to some external resource, such as the graph of your Twitter followers.) We don’t want to constrain ourselves to graphs that are stored in local memory or that can be modified. In any case, specific local implementations of this interface will certainly have methods to add or remove nodes or edges\nJust from the previous definitions, we can already start implementing general methods in graphs, whatever their underlying implementation might be. For example, we can already find the neighborhood and the degree of any given vertex. (Albeit in an extremely slow manner.)\n\n\n\n\n\n\n\n\ngraphs/core.py  See on Github\n\n\n\n# class Graph(...)\n#   ...\n\n    def neighborhood(self, x):\n        for y in self.nodes():\n            if self.adjacent(x, y):\n                yield y\n\n    def degree(self, x) -&gt; int:\n        return len(list(self.neighborhood(x)))\n\n#   ...\n\n\n\n\n\n\n\n\n\nOn Python iterators and generators\n\n\n\nYou may have noticed we used the yield keyword in the neighborhood method up there. In Python, this is the syntax used to define generators.\nIn short, generators allow us to write iterative code without storing intermediate values in a list, instead “returning” each processed node or edge as we find it. The details are quite technical, but you don’t need to understand the full machinery to use them effectively.\nIf this is your first encounter with generators and iterators, check the Appendix B.4 for a quick reference.\n\n\nThis is admittedly the worst way to compute neighborhoods, but hey, at least it works. And it’s simple! “Simple first, optimized later” is our mantra for now.\nIn cases where we have nothing better, this method will do. We will see shortly that we can easily provide more efficient implementations for specific graph representations.\n\nComputational representations of graphs\nThere are several computational representations of graphs, each with their advantages and limitations.\nThe most straightforward representation is called the adjacency list method, which references all neighbors of a given node in a structure associated to that node, such as an array. In Python, we can store a dictionary of nodes mapping to a set of their adjacent nodes. We use a set to store the adjacency information so we can answer as fast as possible whether two nodes are adjacent.\n\n\n\n\n\n\n\n\ngraphs/core.py  See on Github\n\n\n\nclass AdjGraph(Graph):\n    def __init__(self, *nodes) -&gt; None:\n        super().__init__()\n        self._links = {n: set() for n in nodes}\n\n    def nodes(self):\n        return iter(self._links)\n\n    def adjacent(self, x, y) -&gt; bool:\n        return y in self._links[x]\n\n    def neighborhood(self, x):\n        return iter(self._links[x])\n\n    def degree(self, x) -&gt; int:\n        return len(self._links[x])\n\n    def __len__(self):\n        return len(self._links)\n\n    # ... rest of AdjGraph\n\n\n\n\nNote that this implementation allows computing the neighborhood much more directly. It also allows us to dynamically modify the graph by adding vertices and edges.\n\n\n\n\n\n\n\ngraphs/core.py  See on Github\n\n\n\n# class AdjGraph(...)\n#   ...\n\n    def add(self, *nodes):\n        for n in nodes:\n            if n in self._links:\n                return False\n\n            self._links[n] = set()\n\n        return self\n\n    def link(self, x, y):\n        if x == y:\n            raise ValueError(\"Self-links not allowed.\")\n\n        self.add(x)\n        self.add(y)\n        self._links[x].add(y)\n        self._links[y].add(x)\n\n        return self\n\n#   ...\n\n\n\nNotice that we are quite flexible in our modification methods, i.e., we don’t complain if a vertex is already added, and we take care of adding new vertices in link if necessary. This makes it way easier to use our implementation to dynamically construct a graph without taking too much hassle verifying that we aren’t adding duplicated things, paying a minimal overhead in performance.\nHere is how we can use this class to generate our first example graph:\nfrom graphs import AdjGraph\n\nexample_graph = (\n    AdjGraph(*\"abcde\")\n    .link(\"a\",\"b\")\n    .link(\"b\",\"c\")\n    .link(\"c\",\"d\")\n    .link(\"d\",\"a\")\n    .link(\"b\",\"e\")\n    .link(\"c\",\"e\")\n    .link(\"b\",\"d\")\n)\n\n\n\n\n\n\nA bit of syntactic sugar\n\n\n\nIf you noticed that return self at the end of the link method, you may be wondering why is that line there. The reason is so can chain successive calls to link to quickly create custom graphs, like in the previous example.\nThis pattern is often called a “fluent interface” and is very common in object-oriented design. It is not strictly necessary, but it’s a nice little syntactic sugar. We can treat ourselves sometimes, right?\nThere are a few other similar methods in AdjGraph that let you quickly build a graph, adding paths, cycles, and other common structures with a single function call, and using method chaining to combine multiple operations in a single line. We will use and explain them when we need them.\n\n\nAnother commonly used representation is the adjacency matrix method, in which we store a separate structure (like a bidimensional array) that explicitely marks which pairs of nodes are related. The main advantage of this representation is that there is a single place to query or modify for adjacency information. The main disadvantages are the extra wasted space, as the adjacency matrix contains a lot of zeros. (Unless we use a sparse matrix. But that’s for another lesson.) Another disadvantage is the added complexity in computing the neighborhood of a given node. This can be solved with using an extra adjacency list, which would nullify the main advantage of the adjacency matrix implementation.\nFor those reasons, we don’t really gain much with adjacency matrices, and thus we will mostly build on the basic AdjGraph implementation throughout the book. It provides a nice balance between flexibility and performance, although is neither the most flexible nor the most performant implementation possible. When we need to, we will devise other more appropriate implementations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#common-graphs",
    "href": "intro.html#common-graphs",
    "title": "1  Introduction",
    "section": "Common graphs",
    "text": "Common graphs\nThroughout this book we will refer to several common graphs by name. These are graphs that appear over and over in proofs and examples, so it pays to enumerate them briefly here.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A complete graph with 5 nodes.\n\n\n\n\n\n\n\n\n\n\n\n(b) A cycle graph with 5 nodes.\n\n\n\n\n\n\n\n\n\n\n\n(c) A uniform random graph with 5 nodes and p=0.5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) A path graph with 5 nodes.\n\n\n\n\n\n\n\nFigure 1.3: Examples of common graphs.\n\n\n\n\nThe complete graph \\(K_n\\) is the graph of \\(n\\) vertices and all possible edges. It is, by definition, the densest graph one can have with \\(n\\) vertices (see Figure 1.3 (a)).\nThe path graph \\(P_n\\) is a graph composed of \\(n\\) vertices stitched together in sequence, hence it’s a “path”1 (see Figure 1.3 (d)).\nThe cycle graph \\(C_n\\) is a closed-loop of \\(n\\) vertices. So, just like a path, but the first and last vertices are also adjacent (see Figure 1.3 (b)).\nThe random uniform graph \\(U(n,p)\\) is a graph of \\(n\\) vertices, where each pair of vertices has a probability \\(p \\in [0,1]\\) to exist. It is the simplest random graph one can conceive (see Figure 1.3 (c)).\n\n\n\n\n\n\n\\(\\small[\\lambda]\\) Generating common graphs\n\n\n\nIn graphs.generators you will find methods to generate some of the most common types of graphs, including the ones in this section.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#other-types-of-graphs",
    "href": "intro.html#other-types-of-graphs",
    "title": "1  Introduction",
    "section": "Other types of graphs",
    "text": "Other types of graphs\nSo far, we’ve been talking about undirected and unweighted graphs, called like this because there is no specific direction in each edge, and there is no cost associated to edges.\nThus, the edges \\(ab\\) and \\(ba\\) are exactly the same. It would redundant – and incorrect – to mention them both. And each edge is similarly “important”.\n\n\nHowever, in some applications, we need a bit more information. Two specific types of graphs that pop up all the time are directed and weighted graphs, sometimes both in the same problem.\n\nDirected graphs\n\nIn some applications it is interesting to give a direction to edges, and consider that \\(ab\\) is different from \\(ba\\). For example, in modeling transportation networks, sometimes you have single-direction routes. These are called directed graphs. Although they are essential in practical applications, the fundamental theory and the majority of algorithms are almost identical to undirected graphs, so we will only mention them when there’s some relevant difference. Figure 1.4 shows an example.\n\n\n\n\n\n\n\n\nFigure 1.4: Example of a directed graph.\n\n\n\n\n\n\n\nWeighted graphs\n\nIn planning and routing in particular, edges often represent roads or general connections that involve some cost – either a distance, a price, or any other similar notion of cost. In these cases we use weighted graphs, where each edge has an associated number called a weight, and we can ask questions like what is the optimal path between two nodes (where the sum of the weights of the edges involved is smaller).\nWe will see more weighted graphs very soon.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#final-remarks",
    "href": "intro.html#final-remarks",
    "title": "1  Introduction",
    "section": "Final remarks",
    "text": "Final remarks\nNow that we have laid out the foundational theory, we are ready to move on to specific types of graphs and some concrete problems. In the upcoming chapters we will be looking at specific problems and both learn the necessary theory and design clever algorithms to solve them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "We will formalize this concept in the next chapter.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-navigating/intro.html",
    "href": "01-navigating/intro.html",
    "title": "Navigating",
    "section": "",
    "text": "Work in Progress",
    "crumbs": [
      "Navigating"
    ]
  },
  {
    "objectID": "01-navigating/intro.html#work-in-progress",
    "href": "01-navigating/intro.html#work-in-progress",
    "title": "Navigating",
    "section": "",
    "text": "Interested in reading more?\n\n\n\nThis part of the book is still under construction.\nHowever, if you are really interested in it, you can get a sneak peek by supporting this book with an Early Access Pass.\nAn early access pass gives you:\n\nEarly access to all drafts and future versions of the book (including incomplete parts like this one).\nPDF and EPUB formats.\nAccess to a private Discord for discussions about the book.\nAnd a special mention in the Acknowledgment section!\n\nPlease consider getting the Early Access Pass and help us make this project come true.",
    "crumbs": [
      "Navigating"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html",
    "href": "01-navigating/dfs.html",
    "title": "2  The Lurker’s Labyrinth",
    "section": "",
    "text": "Modelling the labyrinth\nThe first step in solving any problem with graphs is, well, turning that problem into a graph problem!\nIn this case, what we must do is find a way out of the labyrinth, so we must decide how to model the labyrinth as a graph such that nodes and edges map to concepts in the labyrinth that are useful for this task. The most natural mapping, and the one you’re probably thinking about, is using edges to model the corridors in the labyrinth, and nodes to model the intersections.\nA possible graph that models a labyrinth is show in Figure 2.1.\nFigure 2.1: A possible graph depicting a labyrinth.\nIn this model, the solution to our problem –finding a way out of the labyrinth– translates into finding a sequence of nodes, each adjacent to next, that take us from the start S to the end E.\nLet’s begin by formalizing this notion of “walking” through the graph, and meet the most important algorithms that will free Ariadne from the Minotaur.",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html#walking-through-a-graph",
    "href": "01-navigating/dfs.html#walking-through-a-graph",
    "title": "2  The Lurker’s Labyrinth",
    "section": "Walking through a graph",
    "text": "Walking through a graph\nThe most important structure in a graph is a sequence of connected vertices. This is called a walk in the general case, where the only restriction is that between any pair of consecutive vertices there is an edge. Let’s look at our usual example graph one more time to see what are we talking about.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A walk\n\n\n\n\n\n\n\n\n\n\n\n(b) A trail\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) A path\n\n\n\n\n\n\n\n\n\n\n\n(d) A cycle\n\n\n\n\n\n\n\nFigure 2.2: Our overused example graph.\n\n\n\nFor example, the sequence \\(a, b, e, d, e, c\\) is a valid walk in our example graph, one that happens to touch all vertices. (But a walk doesn’t have to touch all vertices.) Notice that we can move over the same edge back and forth as we want, so we can extend any walk infinitely.\nIf we never repeat an edge (via backtracking or making a loop), then we have a trail. The previous walk is not trail because we backtrack through \\((d,e)\\). But \\(a, b, e, d, c, e\\) is a valid trail in our example graph, because although \\(e\\) appears twice, we get to it via different edges each time.\nIf we also never repeat a vertex, then we have a path. (Some literature will use path to refer to what we call a trail, and simple path to refer to a path with no repeated vertices). In our example graph, \\(a, b, d, c, e\\) is a path that happens to involve all vertices.\nIf the path loops over from the final vertex back into the first one, like \\(a, b, d, c, e, a\\), then we call it a cycle.\n\nAll trails contain a path \\(\\small(\\Sigma)\\)\nYou might have noticed that the difference between trails and paths is that trails can have small sub-cycles inside them. Intuitively, every time we find one such cycle, we can skip it and continue directly in the “main path”. Thus, it makes sense to think that we can always remove all detours from a trail and extract a path that begins and ends in the same vertices.\nIf we can get from \\(a\\) to \\(b\\) at all, then we must be able to get from \\(a\\) to \\(b\\) without making any unnecessary loops, right? The answer is yes, of course. This is our first theorem, and one we will use often in the rest of the book.\n\nTheorem 2.1 In any graph \\(G\\), if there is a trail from \\(a\\) to \\(b\\), then there is a path from \\(a\\) to \\(b\\).\n\n\nProof. Here is one intuitive, but rigorous demonstration for Theorem 2.1. We will use something called the well-ordering principle1, a fundamental axiom of natural numbers, saying that every non-empty subset of the natural numbers have a least element.\nIn this case, we will consider the set of all possible trails between \\(a\\) and \\(b\\). Since, by the condition of the theorem, there exists a trail from \\(a\\) to \\(b\\), we know this is a non-empty set. Then we can invoke the well-ordering principle and ask for the smallest possible such trail2; let’s denote it by \\(P\\).\n\nWe claim that the shortest trail between \\(a\\) and \\(b\\) must be a path. Why? Here we will use another fundamental tool of logical reasoning: proof by contradiction3.\n\nSuppose that \\(P\\) is not a path. Then, it must contain some internal loop, for otherwise it would be a path and the proof would be done. Thus, if it contains a loop, we can make the trail shorter by removing that loop, and it will still fo from \\(a\\) to \\(b\\). This is in contradiction with the claim that we had the shortest possible trail. Thus, by reductio ad absurdum, the shortest trail must be a path.\nThe proof is almost complete. The only step we slightly overlooked is the claim that the existence of a loop implies that we can make the trail shorter. To airtight this part of the proof, we need to show how to construct a shorter trail by cutting a loop.\n\nLet \\(P = a \\rightarrow^n b\\) be the shortest trail between \\(a\\) and \\(b\\), with length (i.e. number of edges) \\(n = |P| - 1\\). Assume that \\(P\\) has a loop; in other words, there is a vertex \\(x\\), that appears twice inside the trail. (\\(x\\) could be \\(a\\) or \\(b\\) as well.)\nThus, \\(P\\) looks like\n\\[\nP = a \\rightarrow^{k_{1}} x^{(1)} \\rightarrow^{k_{2}} x^{(2)} \\rightarrow^{k_{3}} b,\n\\]\nwhere \\(x^{(i)}\\) indicates the $i $-th time the vertex \\(x\\) appears. Note that \\(k_{1} + k_{2} + k_{3} = n\\), where \\(k_{1} \\geq 0\\), \\(k_{2} &gt; 0\\), and \\(k_{3} \\geq 0\\). That is, the part between \\(x^{(1)}\\) and \\(x^{(2)}\\) must have at least one edge. (In fact, it must have at least two, but we don’t need that.)\nNow we need to construct a valid trail from \\(a\\) to \\(b\\) that is strictly smaller than \\(n\\). In trail \\(P\\) we have two trails \\(a \\rightarrow^{k_1} x\\) and \\(x \\rightarrow^{k_2} b\\) that we can stitch together and make a new trail \\(P’ = a  \\rightarrow^{k_1+k_3} b\\), where the length of \\(P’\\) (equal to \\(k_1+k_3\\)) must be strictly less than \\(n\\) because \\(k_2 &gt; 0\\). \\(\\blacksquare\\)",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html#graph-traversal",
    "href": "01-navigating/dfs.html#graph-traversal",
    "title": "2  The Lurker’s Labyrinth",
    "section": "Graph traversal",
    "text": "Graph traversal\nThe simplest procedure in graphs that involves some notion of “walking” is graph traversal. This is the task of enumerating all nodes in a predefined order by moving through the edges. That is, we don’t want to simply list all nodes, but to order them in a way that uses the graph structure such that subsequent nodes are connected.\nThere are two basic graph traversal algorithms: depth-first search (DFS) and breadth-first search (BFS). Both algorithms are very similar, and will produce a full enumeration of a graph – assuming that all nodes are reachable, a topic we’ll discuss in next chapter. DFS and BFS differ in how they prioritize being eager versus being comprehensive.\n\nAbstract traversals \\(\\small[\\lambda]\\)\nWe will begin by defining how our abstract notion of “search” looks like. To keep things simple, we assume that a search algorithm provides a single method traverse that simply enumerates all edges in the order in which they are discovered.\n\n\n\n\n\n\n\ngraphs/search.py  See on Github\n\n\n\nclass Search:\n    def traverse(self, graph: Graph, root):\n        pass\n\n    def nodes(self, graph: Graph, root):\n        return (y for (x, y) in self.traverse(graph, root))\n\n    # ... extra methods in Search\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe nodes method is just a thing wrapper around traverse that yields the nodes instead the full edges.\n\n\nWhy make this a class? Isn’t this just a method? Well, it’s a bit of mouthful at the moment, for sure. But later, as we explore many search algorithms, we’ll want to compare different strategies. That’s when the search interface will shine.\nPlus, this abstract method allows us to implement a very common search pattern that we’ll’ reuse over and over: searching for an specific set of nodes. (Including a single node.) We can have the general-purpose case that matches any node with a given property, and the special case when we need to find one particular node – e.g., like the exit of the labyrinth.\n\n\n\n\n\n\n\ngraphs/search.py  See on Github\n\n\n\n# class Search(...)\n    #   ...\n\n    def find_any(self, graph: Graph, origin, goal):\n        for node in self.traverse(graph, origin):\n            if goal(node):\n                return True\n\n        return False\n\n    def find(self, graph: Graph, origin, destination):\n        return self.find_any(graph, origin, goal=lambda n: n == destination)\n\n\n\nWith this code in place, we’re ready for some actual search algorithms.",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html#depth-first-search",
    "href": "01-navigating/dfs.html#depth-first-search",
    "title": "2  The Lurker’s Labyrinth",
    "section": "Depth-first search",
    "text": "Depth-first search\nAs the name implies, depth-first search is a graph traversal algorithm that prioritizes going as deep as possible as fast as possible. In our labyrinth analogy, this means turning right until a dead end and backtracking to the last unexplored intersection.\nMore precisely, DFS starts at an arbitrary root node in the graph, then jumps to one of its neighbors, continuing the traversal from there. You can select which neighbor to visit by a random choice, but most commonly one simply defines an order – e.g., the order in which neighbors are listed in the adjacency list.\nIn a practical scenario, this could mean always trying to move south, then east, then north, then west. Of course, you must keep track of which nodes (or intersections in this case) you have already visited. Otherwise you can easily get stuck in a loop.\nThis is how DFS looks like in our sample graph that models the labyrinth problem.\n\n\n\n\n\n\n\n\nFigure 2.3: One possible solution to our labyrinth using DFS.\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding this image\n\n\n\nIn the previous image, we label each edge by a number that indicates the order in which it is discovered by DFS. Thus, edges that have consecutive numbers indicate that DFS traveled along that path. When you see two contiguous edges with non-consecutive numbers, that means DFS backtracked to the corresponding node after getting stuck to explore a different path.\n\n\n\nProgramming DFS \\(\\small[\\lambda]\\)\nLet’s implement DFS! The easiest way is via recursion4: we start at the root node and recursively visit all non-visited neighbors. To make sure we don’t get stuck in a loop, we keep track of the explored nodes throughout all the recursive calls.\nEach iteration returns the edge \\((x, y)\\), where \\(y\\) is the current node under consideration, and \\(x\\) is the “parent” node – that we must also keep track of during recursion.\nHere is the full implementation:\n\n\n\n\n\n\n\ngraphs/search.py  See on Github\n\n\n\nclass DFS(Search):\n    def traverse(self, graph: Graph, root):\n        return self._dfs(graph, root, None, set())\n\n    def _dfs(self, graph: Graph, current, parent, visited: set):\n        yield parent, current\n        visited.add(current)\n\n        for node in graph.neighborhood(current):\n            if node in visited:\n                continue\n\n            yield from self._dfs(graph, node, current, visited)\n\n\n\n\nAs it’s common in recursive methods, we have a public “portal” method traverse that exposes the public arguments, which in turn defers to a private implementation _dfs that takes any additional arguments we need for bookkeeping.\n\n\n\n\n\n\nRecursive iterators\n\n\n\n\n\nYou may be stumped by the use of the yield from syntax at the end of the DFS implementation. This is the Python solution to the problem of building recursive generators that are flattened automatically. It is semantically equivalent to something like:\nfor e in self._dfs(graph, node, current, visited):\n    yield e\nBut yield from does a bit more work to solve some nasty edge cases. To understand why we need this, notice that just using yield at this point would return the whole sub-generator as an element of the parent generator, which is not what you want. Instead, you must unpack the sub-generator and yield each element individually, which is what yield from does under the hood.",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html#finding-your-way-out",
    "href": "01-navigating/dfs.html#finding-your-way-out",
    "title": "2  The Lurker’s Labyrinth",
    "section": "Finding your way out",
    "text": "Finding your way out\nArmed with the arcane knowledge of depth-first search, it is now trivial to exit the labyrinth (provided there is indeed a way out). We just need to run DFS in the start node and, eventually, we will reach the end node. We might have to backtrack once or twice if we are unlucky to pick to wrong turn, but as long as we keep track of every node we visit, and make sure not repeat any of them, the way out is guaranteed to be found.\n\nComputing paths \\(\\small[\\lambda]\\)\nWhile knowing that a goal node exists is useful, we often want to find the actual path that takes us there. Fortunately, our abstract Search strategy can implement this operation easily using the parent from the tuple (parent, node) available in each iteration in the traverse method.\nTo quickly compute paths, we can define a simple structure (Paths) that stores a reference to the parent of each node found during search. With this information, the path between our origin vertex and an arbitrary destination is easy to compute by following the links backwards. That is, we start at the destination node, and iteratively add the parent node to a list, until we find the origin node. Then, we simply reverse the list.\n\n\n\n\n\n\n\ngraphs/search.py  See on Github\n\n\n\nclass Paths:\n    def __init__(self, origin) -&gt; None:\n        self._parents = {}\n        self.origin = origin\n\n    def add(self, node, parent):\n        if node in self._parents:\n            raise ValueError(\"Node already exists\")\n\n        self._parents[node] = parent\n\n    def path(self, destination):\n        path = [destination]\n        node = destination\n\n        while node != self.origin:\n            node = self._parents[node]\n            path.append(node)\n\n        path.reverse()\n        return path\n\n\n\n\n\n\n\n\n\nPaths are origin-dependent\n\n\n\nYou’ll notice we don’t require an origin parameter in the Paths.path method. That is because this structure holds paths precomputed from a fixed origin node, that is, the node from which the search algorithm started.\nIf you need to precompute paths for arbitrary pairs of nodes, there is little you can do other than using a Path instance for each origin node.\n\n\nWith this structure in place, we can add a method to the Search class to compute all paths for a given graph and origin.\n\n\n\n\n\n\n\ngraphs/search.py  See on Github\n\n\n\n# class Search(...)\n    #   ...\n\n    def compute_paths(self, graph: Graph, origin) -&gt; Paths:\n        paths = Paths(origin)\n\n        for parent, node in self.traverse(graph, origin):\n            paths.add(node, parent)\n\n        return paths",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html#analyzing-dfs",
    "href": "01-navigating/dfs.html#analyzing-dfs",
    "title": "2  The Lurker’s Labyrinth",
    "section": "Analyzing DFS",
    "text": "Analyzing DFS\nBefore finishing, let’s turn our attention now to the analysis of this algorithm. In computer science, we are often interested in answering a few critical questions for every algorithm:\n\nDoes the algorithm always work?\nHow fast does it work?\nHow much memory does it need?\nIs there any better algorithm?\n\nThe first question asks about the correctness of the algorithm. An algorithm is only useful if it always works – or if, at least, we can determine beforehand whether it will work or not. In the case of DFS, we can claim with absolute certainty that, if there is a path from origin to destination, the algorithm will eventually find it.\n¿Why? We can give an intuitive justification of correctness for DFS as follows. Since we never repeat any vertex, and we explore all neighbors of the vertices we visit, we must visit every vertex, that is reachable from the origin, eventually.\nNow, there is one massive caveat with DFS. You have no guarantee the path you find is the shortest path from the origin to the destination. Because DFS walks down a path as long as possible, and then never revisits those nodes, you can actually discover the destination first by going throught the long way, instead of taking a shortcut.\n\nThe second and third questions ask about the performance of the algorithm. If we want to compare two algorithms for the same problem, there are almost always at least two obvious dimensions: running time and memory requirement. We usually want the fastest algorithm, provided we have enough memory to run it, right?\nNow, for reasons beyond the scope of this book, it is almost always useless to think in terms of actual time (like seconds) and memory (like megabytes), since the exact values of those properties for any given algorithm will depend on many circumstancial details like the speed of your microprocessor or the programming language you used.\nInstead, to compare two algorithms in the most abstract and fair possible way, we use a relative run-time and memory, that just considers the “size” of the problem. In most cases in this book, the “size” of our problem is something we can approximate by the total number of edges and vertices in our graph.\nThus, we can formulate questions 2 and 3 in terms of, given a graph of \\(n\\) vertices and \\(m\\) edges, how many steps of DFS do we need? You can probably suspect the answer by looking at Figure 2.3 and counting the numbers. You will notice the answer is just the number of edges, since DFS explores the whole graph, and never repeats any path. In general, most search algorithms in this book will take a time that is proportional to the total number of vertices and edges in the graph.\nRegarding memory, we could assume we also need something proportional to the total size of the graph, since we need to mark every visited node, right? Well, that is true only if there are loops in the graph. But if you know beforehand there are no loops, then once you hit a dead end and backtrack, you can forget that whole branch up until the bifurcation. This means that, on average, you will only need enough memory to remember as many nodes as there are in the longest possible path in the graph.\n\nFinally, we can ask if there is any way to discover the exit faster. The general answer is “no”, unless we know something special about the graph. That is, in a general graph, without any extra knowledge, you have absolutely no idea where the exit (or the destination nodes in general) will be, so at worst you’ll have to explore the whole graph.\nHowever, as we will see a few chapters down the road, in many ocassions you do know something extra about the graph – e.g., when driving around a city, you have some sense of which direction your destination is. In these cases, we can often speed-up the search massively with some clever strategies.",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html#final-remarks",
    "href": "01-navigating/dfs.html#final-remarks",
    "title": "2  The Lurker’s Labyrinth",
    "section": "Final remarks",
    "text": "Final remarks\nDepth-First Search is a cornerstone of graph search. Almost all algorithms in this book will either include it as an explicit step, or use it as a building block.\nBesides being extremely simple and elegant to implement, DFS is also the easiest to adapt to a real-life situation. Unlike most other graph search algorithms, DFS only moves to adjacent nodes. So, if you are an agent exploring a graph-like structure in real-life – just like Ariadne – DFS is what you would most likely use.\nThe main caveat of DFS, as we already saw, is that you cannot guarantee that the first time you discover a node, you did so via the shortest path to the origin. In fact, it is easy to come up with examples where you actually reach your destination through the longest possible path –it all depends on how lucky you are selecting which neighboor to visit next.\nNext chapter, we will look at an alternative way to explore a graph that guarantees shortests paths, but it requires that you can teleport to any given node.",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "01-navigating/dfs.html#footnotes",
    "href": "01-navigating/dfs.html#footnotes",
    "title": "2  The Lurker’s Labyrinth",
    "section": "",
    "text": "See https://en.wikipedia.org/wiki/Well-ordering_principle.\nTechnically, we don’t need the full well-ordering principle in this proof because we have a finite set of things, so of course one of them must be the smallest. The well-ordering principle applies also to infinite sets, which is where it becomes really handy.↩︎\nMathematically speaking, we used the well-ordering principle on the lengths of the trails, but that’s in a one-to-many relation with the set of trails. Thus, we can select a trail that has the shortest length.↩︎\nhttps://thepalindrome.org/p/proof-by-induction-and-contradiction↩︎\nAs the saying goes, to understand recursion, you first need to understand recursion.↩︎",
    "crumbs": [
      "Navigating",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Lurker's Labyrinth</span>"
    ]
  },
  {
    "objectID": "02-planning/intro.html",
    "href": "02-planning/intro.html",
    "title": "Planning",
    "section": "",
    "text": "Work in Progress",
    "crumbs": [
      "Planning"
    ]
  },
  {
    "objectID": "02-planning/intro.html#work-in-progress",
    "href": "02-planning/intro.html#work-in-progress",
    "title": "Planning",
    "section": "",
    "text": "Interested in reading more?\n\n\n\nThis part of the book is still under construction.\nHowever, if you are really interested in it, you can get a sneak peek by supporting this book with an Early Access Pass.\nAn early access pass gives you:\n\nEarly access to all drafts and future versions of the book (including incomplete parts like this one).\nPDF and EPUB formats.\nAccess to a private Discord for discussions about the book.\nAnd a special mention in the Acknowledgment section!\n\nPlease consider getting the Early Access Pass and help us make this project come true.",
    "crumbs": [
      "Planning"
    ]
  },
  {
    "objectID": "03-socializing/intro.html",
    "href": "03-socializing/intro.html",
    "title": "Socializing",
    "section": "",
    "text": "Work in Progress",
    "crumbs": [
      "Socializing"
    ]
  },
  {
    "objectID": "03-socializing/intro.html#work-in-progress",
    "href": "03-socializing/intro.html#work-in-progress",
    "title": "Socializing",
    "section": "",
    "text": "Interested in reading more?\n\n\n\nThis part of the book is still under construction.\nHowever, if you are really interested in it, you can get a sneak peek by supporting this book with an Early Access Pass.\nAn early access pass gives you:\n\nEarly access to all drafts and future versions of the book (including incomplete parts like this one).\nPDF and EPUB formats.\nAccess to a private Discord for discussions about the book.\nAnd a special mention in the Acknowledgment section!\n\nPlease consider getting the Early Access Pass and help us make this project come true.",
    "crumbs": [
      "Socializing"
    ]
  },
  {
    "objectID": "04-reasoning/intro.html",
    "href": "04-reasoning/intro.html",
    "title": "Reasoning",
    "section": "",
    "text": "Work in Progress",
    "crumbs": [
      "Reasoning"
    ]
  },
  {
    "objectID": "04-reasoning/intro.html#work-in-progress",
    "href": "04-reasoning/intro.html#work-in-progress",
    "title": "Reasoning",
    "section": "",
    "text": "Interested in reading more?\n\n\n\nThis part of the book is still under construction.\nHowever, if you are really interested in it, you can get a sneak peek by supporting this book with an Early Access Pass.\nAn early access pass gives you:\n\nEarly access to all drafts and future versions of the book (including incomplete parts like this one).\nPDF and EPUB formats.\nAccess to a private Discord for discussions about the book.\nAnd a special mention in the Acknowledgment section!\n\nPlease consider getting the Early Access Pass and help us make this project come true.",
    "crumbs": [
      "Reasoning"
    ]
  },
  {
    "objectID": "05-drawing/intro.html",
    "href": "05-drawing/intro.html",
    "title": "Laying out",
    "section": "",
    "text": "Work in Progress",
    "crumbs": [
      "Laying out"
    ]
  },
  {
    "objectID": "05-drawing/intro.html#work-in-progress",
    "href": "05-drawing/intro.html#work-in-progress",
    "title": "Laying out",
    "section": "",
    "text": "Interested in reading more?\n\n\n\nThis part of the book is still under construction.\nHowever, if you are really interested in it, you can get a sneak peek by supporting this book with an Early Access Pass.\nAn early access pass gives you:\n\nEarly access to all drafts and future versions of the book (including incomplete parts like this one).\nPDF and EPUB formats.\nAccess to a private Discord for discussions about the book.\nAnd a special mention in the Acknowledgment section!\n\nPlease consider getting the Early Access Pass and help us make this project come true.",
    "crumbs": [
      "Laying out"
    ]
  },
  {
    "objectID": "appendix/math.html",
    "href": "appendix/math.html",
    "title": "Appendix A — A Primer on Logic",
    "section": "",
    "text": "Sets",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Logic</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#logical-proofs",
    "href": "appendix/math.html#logical-proofs",
    "title": "Appendix A — A Primer on Logic",
    "section": "Logical proofs",
    "text": "Logical proofs\n\nProof by Induction\n\n\nProof by Contradiction",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Logic</span>"
    ]
  },
  {
    "objectID": "appendix/math.html#the-well-ordering-principle",
    "href": "appendix/math.html#the-well-ordering-principle",
    "title": "Appendix A — A Primer on Logic",
    "section": "The Well-ordering Principle",
    "text": "The Well-ordering Principle",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Logic</span>"
    ]
  },
  {
    "objectID": "appendix/python.html",
    "href": "appendix/python.html",
    "title": "Appendix B — A Primer on Python",
    "section": "",
    "text": "Basic syntax",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "appendix/python.html#functions",
    "href": "appendix/python.html#functions",
    "title": "Appendix B — A Primer on Python",
    "section": "Functions",
    "text": "Functions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "appendix/python.html#classes",
    "href": "appendix/python.html#classes",
    "title": "Appendix B — A Primer on Python",
    "section": "Classes",
    "text": "Classes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "appendix/python.html#sec-python-iterators",
    "href": "appendix/python.html#sec-python-iterators",
    "title": "Appendix B — A Primer on Python",
    "section": "Generators and Iterators",
    "text": "Generators and Iterators",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "appendix/core.html",
    "href": "appendix/core.html",
    "title": "Appendix C — The hitchhiker-graphs Python Package",
    "section": "",
    "text": "This appendix is dedicated to the hitchhiker-graphs Python package. You are free to skip it if you don’t care about the coding part of the book.\nThe hitchhikers-graph package contains the main code used in this book, including class definitions for all the graph types and functions for all search algorithms. It does not contain scripting code that we used to illustrate how the algorithms run.\nTo install the package, just type:\n$ pip install hitchhiker-graphs\nOnce installation is ready, you can import the AdjGraph class and start doing fun stuff.\n\nfrom graphs import AdjGraph\n\ng = AdjGraph().cycle(1,2,3,4,5).link(2,4).link(3,5)\n\nWhich generates the following graph:\n\n\n\n\n\nA simple graph",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The `hitchhiker-graphs` Python Package</span>"
    ]
  }
]